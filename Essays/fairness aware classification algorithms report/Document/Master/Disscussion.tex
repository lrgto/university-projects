\section{Discussion}
\label{Discussion Section}

Examining the 2 Naïve Bayes models, the model would work well with small data as it can only split the data set into favoured values and discriminated values however the values in $M_-$ contain all discriminated values, it doesn't specify the types of discrimination i.e. age, gender, race.... however it can save run time by not having to filter through all these types or classes of discrimination. An interesting negative to evaluated is that the base Naïve Bayes algorithm doesn't have a dependence of A on S but in this case it must as the data is split and balanced to form a total of S. The results of testing of the 2 Naïve Bayes in \cite{Naive} proved that the it "it achieves high accuracy scores with zero discrimination, and has the smallest dependency on S." \cite{Naive}. The utilisation of splitting the data and learning two separate models showed insightful as $S$ was calculated not from C but from the attributes $A_i$. \\

For the algorithm trees, a formula must be chosen from the five formulae to which truly represents the data. When dealing with large amounts of data, trees that implement a simple splitting criteria (IGC) offered a lack of improvement to accuracy, the IGC+IGS only improves the system when relying on an external variable, this was in the form on a relabelling system in \cite{Tree}. The decision tree algorithm stated in the results that "From the results of our experiments we draw the fol- lowing conclusions: (1) Our proposed methods give high accuracy and low discrimination scores when applied to non- discriminatory test data. In this scenario, our methods are the best choice, even if we are only concerned with accuracy." \cite{Tree}. This states that this method is effective when the training data is discriminatory and the test data is non-discriminatory, is shows with the high accuracy and low discrimination scores implying that this algorithm suites large data sets that are pre-complied into bias data-sets. \\